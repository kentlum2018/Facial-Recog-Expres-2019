{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FERProject.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kentlum2018/Facial-Recog-Expres-2019/blob/master/FERProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML2_TNqkfMUH",
        "colab_type": "code",
        "outputId": "88b27de0-84d3-4843-dd6b-5f70d57b0287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RctfIcOFM1ZX",
        "colab_type": "text"
      },
      "source": [
        "lalalalalala\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dog2j9qOfpG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.models import Sequential #Initialise our neural network model as a sequential network\n",
        "from keras.layers import Conv2D #Convolution operation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Activation#Applies activation function\n",
        "from keras.layers import Dropout#Prevents overfitting by randomly converting few outputs to zero\n",
        "from keras.layers import MaxPooling2D # Maxpooling function\n",
        "from keras.layers import Flatten # Converting 2D arrays into a 1D linear vector\n",
        "from keras.layers import Dense # Regular fully connected neural network\n",
        "from keras import optimizers\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlsDOPfifNXP",
        "colab_type": "code",
        "outputId": "a6da7282-81b1-417f-f74b-eb33d72a9697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSqlCj-Cf8Mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(dataset_path):\n",
        "  \n",
        "  #classes = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprsie', 'Neutral']  #We will be dealing with seven different types of emotions.\n",
        "\n",
        "  data = []\n",
        "  test_data = []\n",
        "  test_labels = []\n",
        "  labels =[]\n",
        "\n",
        "  with open(dataset_path, 'r') as file:\n",
        "      for line_no, line in enumerate(file.readlines()):\n",
        "          if 0 < line_no <= 35887:\n",
        "            curr_class, line, set_type = line.split(',')\n",
        "            image_data = np.asarray([int(x) for x in line.split()]).reshape(48, 48)#Creating a list out of the string then converting it into a 2-Dimensional numpy array.\n",
        "            image_data =image_data.astype(np.uint8)/255.0\n",
        "            \n",
        "            if (set_type.strip() == 'PrivateTest'):\n",
        "              \n",
        "              test_data.append(image_data)\n",
        "              test_labels.append(curr_class)\n",
        "            else:\n",
        "              data.append(image_data)\n",
        "              labels.append(curr_class)\n",
        "      \n",
        "      test_data = np.expand_dims(test_data, -1)\n",
        "      test_labels = to_categorical(test_labels, num_classes = 7)\n",
        "      data = np.expand_dims(data, -1)   \n",
        "      labels = to_categorical(labels, num_classes = 7)\n",
        "    \n",
        "      return np.array(data), np.array(labels), np.array(test_data), np.array(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r94tgQGIgYk9",
        "colab_type": "code",
        "outputId": "2a4a517c-76de-4918-a7ec-f4705b847c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "dataset_path = \"/content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Data/fer2013.csv\" \n",
        "train_data, train_labels, test_data, test_labels = load_data(dataset_path)\n",
        "#train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size = test_size,random_state = seed)\n",
        "\n",
        "print(\"Number of images in Training set:\", len(train_data))\n",
        "print(\"Number of images in Test set:\", len(test_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in Training set: 32298\n",
            "Number of images in Test set: 3589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE3ysJDzgchl",
        "colab_type": "code",
        "outputId": "e409bcbb-099a-48b7-ce77-4e1d5f55f5c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4683
        }
      },
      "source": [
        "#######HYPERPARAMATERS###########\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "#################################\n",
        "\n",
        "model = Sequential()\n",
        "    \n",
        "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1), kernel_regularizer=l2(0.01)))\n",
        "model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "    \n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "    \n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "    \n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, (3, 3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "    \n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "adam = optimizers.Adam(lr = learning_rate)\n",
        "model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    \n",
        "print(model.summary())\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
        "early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=6, mode='auto')\n",
        "checkpointer = ModelCheckpoint('/content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "model.fit(\n",
        "          train_data,\n",
        "          train_labels,\n",
        "          epochs = epochs,\n",
        "          batch_size = batch_size,\n",
        "          validation_split = 0.2,\n",
        "          shuffle = True,\n",
        "          callbacks=[lr_reducer, checkpointer, early_stopper]\n",
        "          )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 46, 46, 64)        640       \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 46, 46, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 46, 46, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 23, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 23, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 23, 23, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 23, 23, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 23, 23, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 23, 23, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 23, 23, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 23, 23, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 11, 11, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 11, 11, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 11, 11, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 5, 5, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 5, 5, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 5, 5, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 5, 5, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 5, 5, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 5, 5, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 7)                 455       \n",
            "=================================================================\n",
            "Total params: 9,014,727\n",
            "Trainable params: 9,009,223\n",
            "Non-trainable params: 5,504\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 25838 samples, validate on 6460 samples\n",
            "Epoch 1/100\n",
            "25838/25838 [==============================] - 34s 1ms/step - loss: 2.1475 - acc: 0.1962 - val_loss: 1.8789 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.87894, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 2/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8675 - acc: 0.2355 - val_loss: 1.8420 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.87894 to 1.84205, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 3/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8391 - acc: 0.2480 - val_loss: 1.8274 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.84205 to 1.82738, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 4/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8291 - acc: 0.2504 - val_loss: 1.8149 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.82738 to 1.81492, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 5/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8231 - acc: 0.2501 - val_loss: 1.8211 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.81492\n",
            "Epoch 6/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8175 - acc: 0.2517 - val_loss: 1.8115 - val_acc: 0.2491\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.81492 to 1.81150, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 7/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8160 - acc: 0.2514 - val_loss: 1.8064 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.81150 to 1.80644, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 8/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8092 - acc: 0.2524 - val_loss: 1.7966 - val_acc: 0.2489\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.80644 to 1.79663, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 9/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.8026 - acc: 0.2524 - val_loss: 1.8070 - val_acc: 0.2492\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.79663\n",
            "Epoch 10/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.7966 - acc: 0.2585 - val_loss: 1.8099 - val_acc: 0.2502\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.79663\n",
            "Epoch 11/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.7690 - acc: 0.2746 - val_loss: 1.7980 - val_acc: 0.2517\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.79663\n",
            "Epoch 12/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.7229 - acc: 0.2992 - val_loss: 1.8095 - val_acc: 0.2551\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.79663\n",
            "Epoch 13/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.6711 - acc: 0.3145 - val_loss: 1.5905 - val_acc: 0.3407\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.79663 to 1.59050, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 14/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.6206 - acc: 0.3368 - val_loss: 1.6249 - val_acc: 0.3144\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.59050\n",
            "Epoch 15/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.5703 - acc: 0.3725 - val_loss: 1.5396 - val_acc: 0.3831\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.59050 to 1.53958, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 16/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.5199 - acc: 0.3930 - val_loss: 1.4928 - val_acc: 0.3981\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.53958 to 1.49283, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 17/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.4904 - acc: 0.4099 - val_loss: 1.4810 - val_acc: 0.4084\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.49283 to 1.48096, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 18/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.4580 - acc: 0.4219 - val_loss: 1.4562 - val_acc: 0.4257\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.48096 to 1.45616, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 19/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.4234 - acc: 0.4332 - val_loss: 1.4185 - val_acc: 0.4268\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.45616 to 1.41854, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 20/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.4051 - acc: 0.4378 - val_loss: 1.4179 - val_acc: 0.4384\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.41854 to 1.41793, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 21/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.3845 - acc: 0.4424 - val_loss: 1.3765 - val_acc: 0.4449\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.41793 to 1.37649, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 22/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.3653 - acc: 0.4483 - val_loss: 1.5169 - val_acc: 0.3978\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.37649\n",
            "Epoch 23/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.3458 - acc: 0.4577 - val_loss: 1.3351 - val_acc: 0.4824\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.37649 to 1.33514, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 24/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.3362 - acc: 0.4663 - val_loss: 1.4213 - val_acc: 0.4698\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.33514\n",
            "Epoch 25/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.3096 - acc: 0.4881 - val_loss: 1.2769 - val_acc: 0.5093\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.33514 to 1.27695, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 26/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.2976 - acc: 0.5035 - val_loss: 1.2645 - val_acc: 0.5149\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.27695 to 1.26455, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 27/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.2698 - acc: 0.5170 - val_loss: 1.2875 - val_acc: 0.5118\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.26455\n",
            "Epoch 28/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.2386 - acc: 0.5253 - val_loss: 1.2229 - val_acc: 0.5333\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.26455 to 1.22294, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 29/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.2216 - acc: 0.5376 - val_loss: 1.1959 - val_acc: 0.5588\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.22294 to 1.19586, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 30/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1982 - acc: 0.5460 - val_loss: 1.3245 - val_acc: 0.5121\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.19586\n",
            "Epoch 31/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1814 - acc: 0.5517 - val_loss: 1.3436 - val_acc: 0.4907\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.19586\n",
            "Epoch 32/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1534 - acc: 0.5669 - val_loss: 1.1932 - val_acc: 0.5534\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.19586 to 1.19318, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 33/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1423 - acc: 0.5717 - val_loss: 1.1653 - val_acc: 0.5698\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.19318 to 1.16530, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 34/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.1173 - acc: 0.5826 - val_loss: 1.2112 - val_acc: 0.5505\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.16530\n",
            "Epoch 35/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0953 - acc: 0.5923 - val_loss: 1.1855 - val_acc: 0.5601\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.16530\n",
            "Epoch 36/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0750 - acc: 0.6039 - val_loss: 1.1448 - val_acc: 0.5732\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.16530 to 1.14484, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 37/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0635 - acc: 0.6077 - val_loss: 1.1633 - val_acc: 0.5797\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.14484\n",
            "Epoch 38/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0423 - acc: 0.6151 - val_loss: 1.1207 - val_acc: 0.5992\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.14484 to 1.12072, saving model to /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\n",
            "Epoch 39/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0220 - acc: 0.6273 - val_loss: 1.3314 - val_acc: 0.5229\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.12072\n",
            "Epoch 40/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 1.0070 - acc: 0.6330 - val_loss: 1.2347 - val_acc: 0.5741\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.12072\n",
            "Epoch 41/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9934 - acc: 0.6362 - val_loss: 1.1430 - val_acc: 0.5924\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.12072\n",
            "Epoch 42/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9641 - acc: 0.6505 - val_loss: 1.2021 - val_acc: 0.5607\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.12072\n",
            "Epoch 43/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9365 - acc: 0.6557 - val_loss: 1.2147 - val_acc: 0.5697\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.12072\n",
            "Epoch 44/100\n",
            "25838/25838 [==============================] - 32s 1ms/step - loss: 0.9525 - acc: 0.6534 - val_loss: 1.2758 - val_acc: 0.5655\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.12072\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5ab51df9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQi0jdvvmSpD",
        "colab_type": "code",
        "outputId": "9d66af6f-e118-4a77-faf6-4190d9cec216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted_test_labels = np.argmax(model.predict(test_data), axis=1)\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "print (\"Accuracy score = \", accuracy_score(test_labels, predicted_test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score =  0.5658957926999164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lkkBoIhE9Ce",
        "colab_type": "text"
      },
      "source": [
        "The test data set is being processed here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4k8yWw3pd7E",
        "colab_type": "code",
        "outputId": "b3c37177-6725-4639-8225-710c5eacf1f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import model_from_json\n",
        "model_json = model.to_json()\n",
        "with open(\"/content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"/content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gNN2dd_-D1V",
        "colab_type": "code",
        "outputId": "81a6f094-f5b1-4fcf-ffa3-fe8c87f5570c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import h5py\n",
        "\n",
        "filename = \"/content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5\"\n",
        "# https://drive.google.com/open?id=18ZT7ARqUn5jNNFYe9y3lhBA-KK6IlaMG\n",
        "#  /content/gdrive/My Drive/Colab Notebooks/Emotion Recognition/Model/weights.hd5 \n",
        "h5 = h5py.File(filename,'r')\n",
        "print(h5)\n",
        "\n",
        "# futures_data = h5['futures_data']  # VSTOXX futures data\n",
        "# options_data = h5['options_data']  # VSTOXX call option data\n",
        "\n",
        "# h5.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<HDF5 file \"weights.hd5\" (mode r)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdr3WJk5plRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB7btM8POfuk",
        "colab_type": "text"
      },
      "source": [
        "How to create a test file....self and classmates?\n",
        "Why is facial recognition important? mental health, train to gamble, product marketing and see if this is something that ppl want? tailor it after looking at the person..product development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUESbYZRNkz4",
        "colab_type": "text"
      },
      "source": [
        "Notes:\n",
        "How to read hd5 file?\n",
        "Visual of CNN\n",
        "Change padding\n",
        "Change batch size\n",
        "Loss vs accuracy"
      ]
    }
  ]
}