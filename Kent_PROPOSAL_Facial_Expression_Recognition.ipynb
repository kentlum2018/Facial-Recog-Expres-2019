{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement- Learn facial expressions \n",
    "Use a labeled facial expression dataset to do 'deep learning' and classify unlabeled facial expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing dependencies\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from keras.models import model_from_json\n",
    "from pathlib import Path\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matrix_util import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set - Muxspace Facial Expressions\n",
    "Public Unprocessed Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing CSV file containing image labels and corresponding image file names\n",
    "\n",
    "path_to_labels=\"data/legend.csv\"\n",
    "\n",
    "data=pd.read_csv(path_to_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation\n",
    "Review the datasets and remove images that are not a uniform size, change the case to all lower case, default images to grayscale, relabel as needed\n",
    "\n",
    "Created a new file of uniform image sizes, 350 by 350 pixels\n",
    "\n",
    "Relabeled Sadness/SADNESS to Sad \n",
    "\n",
    "Limited the data to Happy, Sad, Anger, and Neutral\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All available emotions in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger',\n",
       " 'surprise',\n",
       " 'disgust',\n",
       " 'fear',\n",
       " 'neutral',\n",
       " 'happiness',\n",
       " 'sadness',\n",
       " 'contempt',\n",
       " 'NEUTRAL',\n",
       " 'SADNESS',\n",
       " 'DISGUST',\n",
       " 'FEAR',\n",
       " 'SURPRISE',\n",
       " 'ANGER',\n",
       " 'HAPPINESS',\n",
       " 'sad']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=list(data['emotion'].unique())\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user.id</th>\n",
       "      <th>image</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14185</th>\n",
       "      <td>906</td>\n",
       "      <td>Tim_Pawlenty_0001.jpg</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14186</th>\n",
       "      <td>906</td>\n",
       "      <td>Tim_Robbins_0001.jpg</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14187</th>\n",
       "      <td>906</td>\n",
       "      <td>Tim_Robbins_0002.jpg</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14188</th>\n",
       "      <td>906</td>\n",
       "      <td>Tim_Robbins_0003.jpg</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14189</th>\n",
       "      <td>906</td>\n",
       "      <td>Tim_Robbins_0004.jpg</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user.id                  image    emotion\n",
       "14185     906  Tim_Pawlenty_0001.jpg    neutral\n",
       "14186     906   Tim_Robbins_0001.jpg    neutral\n",
       "14187     906   Tim_Robbins_0002.jpg    neutral\n",
       "14188     906   Tim_Robbins_0003.jpg    neutral\n",
       "14189     906   Tim_Robbins_0004.jpg  happiness"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# record cnt is 14189\n",
    "\n",
    "data.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower casing all the values in the emotion column\n",
    "\n",
    "data['emotion'] = [str(i).lower() for i in data[\"emotion\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-labelling all the columns labelled sad, with sadness for consistency\n",
    "\n",
    "data['emotion']=data['emotion'].replace(to_replace='sadness', value='sad', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger',\n",
       " 'surprise',\n",
       " 'disgust',\n",
       " 'fear',\n",
       " 'neutral',\n",
       " 'happiness',\n",
       " 'sad',\n",
       " 'contempt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=list(data['emotion'].unique())\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 4 emotions of choice i.e. anger, neutral, happiness, sad \n",
    "# to increase accuracy for the model\n",
    "\n",
    "restricted_to_specific_emo=data.loc[data.emotion.isin([\"anger\",\"neutral\",\"happiness\",\"sad\"])].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'neutral', 'happiness', 'sad']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels we are going to use\n",
    "\n",
    "using_labels=list(restricted_to_specific_emo['emotion'].unique())\n",
    "using_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE OPEN CV2 to generate array for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the column name \"image\" from the dataframe labelled data and converting to list\n",
    "\n",
    "img_file_names=list(restricted_to_specific_emo[\"image\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the array value of each file\n",
    "\n",
    "grayscale_image_array=[]\n",
    "\n",
    "for i in range(len(img_file_names)):\n",
    "    img_path = f\"images/{img_file_names[i]}\"\n",
    "    img = cv2.imread(img_path,0)\n",
    "    grayscale_image_array.append(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>num_images</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_rows</th>\n",
       "      <th>num_cols</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <th>350</th>\n",
       "      <td>12823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <th>73</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <th>80</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <th>58</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   num_images\n",
       "num_rows num_cols            \n",
       "350      350            12823\n",
       "37       27                 3\n",
       "91       73                 3\n",
       "99       80                 2\n",
       "73       58                 2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of images for each image size\n",
    "\n",
    "(\n",
    "    pd.DataFrame.from_records(\n",
    "        [img.shape for img in grayscale_image_array],\n",
    "        columns=['num_rows', 'num_cols'])\n",
    "    .groupby(['num_rows', 'num_cols'])\n",
    "    .size()\n",
    "    .rename('num_images')\n",
    "    .to_frame()\n",
    "    .sort_values(by=\"num_images\", ascending=False)\n",
    ").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-98c2e1b14449>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-98c2e1b14449>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    columns=['num_rows', 'num_cols'])\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pd.crosstab(\n",
    "    ([img.shape for img in grayscale_image_array],\n",
    "    columns=['num_rows', 'num_cols'])\n",
    "    .groupby(['num_rows', 'num_cols'])\n",
    "    .size()\n",
    "    .rename('num_images')\n",
    "    .to_frame()\n",
    "    .sort_values(by=\"num_images\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the array values of all the images of size 350 by 350 and their corresponding emotion values and\n",
    "# appending to new lists\n",
    "\n",
    "\n",
    "grayscale_image_array_only_350by350=[]\n",
    "emotions_only_350by350=[]\n",
    "\n",
    "\n",
    "for i in range(len(grayscale_image_array)):\n",
    "    img = grayscale_image_array[i]\n",
    "    emotion = restricted_to_specific_emo.emotion[i]\n",
    "    \n",
    "    if img.shape == (350, 350):\n",
    "        grayscale_image_array_only_350by350.append(img)\n",
    "        emotions_only_350by350.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12823\n",
      "12823\n"
     ]
    }
   ],
   "source": [
    "# Checking the size of the new lists\n",
    "\n",
    "print(len(grayscale_image_array_only_350by350))\n",
    "print(len(emotions_only_350by350))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the orientation of the image in multiple ways to create a more robust data set which will then be used \n",
    "# to train and test the model\n",
    "\n",
    "images_reflected_horizontally=[]\n",
    "\n",
    "images_reflected_vertically=[]\n",
    "\n",
    "images_rotated_right=[]\n",
    "\n",
    "images_rotated_left=[]\n",
    "\n",
    "images_rotated_180=[]\n",
    "\n",
    "\n",
    "for i in range(len(grayscale_image_array_only_350by350)):\n",
    "    \n",
    "    # grabs images and assigns to img\n",
    "    img=grayscale_image_array_only_350by350[i]\n",
    "    \n",
    "    #reflects the image horizontally and appends to images_reflected_horizontally list\n",
    "    images_reflected_horizontally.append(reflect_horizontally(img))\n",
    "    \n",
    "    #reflects the image vertically and appends to images_reflected_vertically list\n",
    "    images_reflected_vertically.append(reflect_vertically(img))\n",
    "    \n",
    "    #rotates the image right and appends to images_rotated_right list\n",
    "    images_rotated_right.append(rotate_right(img))\n",
    "    \n",
    "    #rotates the image left and appends to images_rotated_left list\n",
    "    images_rotated_left.append(rotate_left(img))\n",
    "    \n",
    "    #rotates the image 180 degrees and appends to images_rotated_180 list\n",
    "    images_rotated_180.append(rotate_180(img))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work :\n",
    "Feature Engineering:\n",
    "\n",
    "Expand the image database for additional expressions\n",
    "\n",
    "additional faces\n",
    "\n",
    "additional angles (profile, off center, etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is my final list of images (X) in various orientations\n",
    "\n",
    "final_list_images= (\n",
    "    grayscale_image_array_only_350by350 + \n",
    "    images_reflected_horizontally + \n",
    "    images_reflected_vertically +\n",
    "    images_rotated_right +\n",
    "    images_rotated_left+\n",
    "    images_rotated_180\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the emotion labels (y) corresponding to the X images\n",
    "\n",
    "final_list_emotions= emotions_only_350by350*6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is done to get y and X \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting list grayscale_image_array_only_350by350 to array and reshaping the dimensions\n",
    "\n",
    "X = np.concatenate([\n",
    "    img.reshape(1,350,350,1)\n",
    "    for img in\n",
    "    final_list_images\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the text label into a number between 0 and 3 (because 4 labels) and converting this to \n",
    "# an array and reshaping\n",
    "\n",
    "label_to_number_map = {\n",
    "    label: idx for idx, label in enumerate(using_labels)}\n",
    "\n",
    "emo_numbers = [\n",
    "    label_to_number_map[emo] for emo in final_list_emotions\n",
    "]\n",
    "emo_numbers_array = np.array(emo_numbers).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "Deep Learning - Convolutional Neural Networks (CNN)\n",
    "Model/Fit/Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the number into one hot encoding resulting in a matrix on size 12183 X 4\n",
    "\n",
    "y = keras.utils.to_categorical(\n",
    "    emo_numbers_array,\n",
    "    len(using_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a trained test split from the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(2, (3, 3), padding='same', input_shape=(350,350,1), activation=\"relu\"))\n",
    "model.add(Conv2D(2, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(4, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(4, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(8, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(8, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(16, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3)\n",
    "\n",
    "early_stopper = EarlyStopping(monitor='val_acc', min_delta=0, patience=6, mode='auto')\n",
    "\n",
    "checkpointer = ModelCheckpoint('weights.hd5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size=32,\n",
    "          epochs=14,\n",
    "          validation_data=(X_test,y_test),\n",
    "          shuffle=True,\n",
    "          callbacks=[lr_reducer, checkpointer, early_stopper]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = model.to_json()\n",
    "f = Path(\"model_structure.json\")\n",
    "f.write_text(model_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save neural network's trained weights\n",
    "model.save_weights(\"model_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHW Read it Back\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from pathlib import Path\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Load the json file that contains the model's structure\n",
    "f2 = Path(\"model_structure.json\")\n",
    "model_structure2 = f2.read_text()\n",
    "\n",
    "# Recreate the Keras model object from the json data\n",
    "model2 = model_from_json(model_structure2)\n",
    "\n",
    "# Re-load the model's trained weights\n",
    "model2.load_weights(\"model_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from pathlib import Path\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Load the json file that contains the model's structure\n",
    "f = Path(\"model_structure.json\")\n",
    "model_structure = f.read_text()\n",
    "\n",
    "\n",
    "\n",
    "# Recreate the Keras model object from the json data\n",
    "model = model_from_json(model_structure)\n",
    "\n",
    "# Re-load the model's trained weights\n",
    "model.load_weights(\"model_weights.h5\")\n",
    "\n",
    "# Load an image file to test, resizing it to 350 X 350 pixels (as required by this model)\n",
    "img = image.load_img(\"Aaron_Sorkin_0002.jpg\", target_size=(350, 350))\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "image_to_test = image.img_to_array(img)\n",
    "\n",
    "# Convert the image equivalent numpy array from 3D to grayscale\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Add a fourth dimension to the image (since Keras expects a list of images, not a single image)\n",
    "# list_of_images = np.expand_dims(image_to_test, axis=0)\n",
    "\n",
    "# # Make a prediction using the model\n",
    "# results = model.predict(list_of_images)\n",
    "\n",
    "# # Since we are only testing one image, we only need to check the first result\n",
    "# single_result = results[0]\n",
    "\n",
    "# # We will get a likelihood score for all 10 possible classes. Find out which class had the highest score.\n",
    "# most_likely_class_index = int(np.argmax(single_result))\n",
    "# class_likelihood = single_result[most_likely_class_index]\n",
    "\n",
    "# # Get the name of the most likely class\n",
    "# class_label = class_labels[most_likely_class_index]\n",
    "\n",
    "# # Print the result\n",
    "# print(\"This is image is a {} - Likelihood: {:2f}\".format(class_label, class_likelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
